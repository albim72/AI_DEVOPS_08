{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-08T09:40:07.712618Z",
          "iopub.status.busy": "2024-05-08T09:40:07.711874Z",
          "iopub.status.idle": "2024-05-08T09:40:07.720179Z",
          "shell.execute_reply": "2024-05-08T09:40:07.719585Z"
        },
        "id": "as4OTe2ukSqm"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  import colab\n",
        "  !pip install --upgrade pip\n",
        "except:\n",
        "  pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-08T09:40:07.723718Z",
          "iopub.status.busy": "2024-05-08T09:40:07.723172Z",
          "iopub.status.idle": "2024-05-08T09:40:18.578900Z",
          "shell.execute_reply": "2024-05-08T09:40:18.578016Z"
        },
        "id": "iyQtljP-qPHY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fca89814-a2b0-4ccf-caa8-05cdc4bba917"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tfx in /usr/local/lib/python3.10/dist-packages (1.15.1)\n",
            "Requirement already satisfied: ml-pipelines-sdk==1.15.1 in /usr/local/lib/python3.10/dist-packages (from tfx) (1.15.1)\n",
            "Requirement already satisfied: absl-py<2.0.0,>=0.9 in /usr/local/lib/python3.10/dist-packages (from tfx) (1.4.0)\n",
            "Requirement already satisfied: ml-metadata<1.16.0,>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from tfx) (1.15.0)\n",
            "Requirement already satisfied: packaging>=22 in /usr/local/lib/python3.10/dist-packages (from tfx) (24.1)\n",
            "Requirement already satisfied: portpicker<2,>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from tfx) (1.5.2)\n",
            "Requirement already satisfied: protobuf<5,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tfx) (3.20.3)\n",
            "Requirement already satisfied: docker<5,>=4.1 in /usr/local/lib/python3.10/dist-packages (from tfx) (4.4.4)\n",
            "Requirement already satisfied: google-apitools<1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tfx) (0.5.31)\n",
            "Requirement already satisfied: google-api-python-client<2,>=1.8 in /usr/local/lib/python3.10/dist-packages (from tfx) (1.12.11)\n",
            "Requirement already satisfied: jinja2<4,>=2.7.3 in /usr/local/lib/python3.10/dist-packages (from tfx) (3.1.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=3.10.0.2 in /usr/local/lib/python3.10/dist-packages (from tfx) (4.12.2)\n",
            "Requirement already satisfied: apache-beam<3,>=2.47 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (2.58.1)\n",
            "Requirement already satisfied: attrs<24,>=19.3.0 in /usr/local/lib/python3.10/dist-packages (from tfx) (23.2.0)\n",
            "Requirement already satisfied: click<9,>=7 in /usr/local/lib/python3.10/dist-packages (from tfx) (8.1.7)\n",
            "Requirement already satisfied: google-api-core<3 in /usr/local/lib/python3.10/dist-packages (from tfx) (2.19.1)\n",
            "Requirement already satisfied: google-cloud-aiplatform<2,>=1.6.2 in /usr/local/lib/python3.10/dist-packages (from tfx) (1.59.0)\n",
            "Requirement already satisfied: google-cloud-bigquery<4,>=3 in /usr/local/lib/python3.10/dist-packages (from tfx) (3.25.0)\n",
            "Requirement already satisfied: grpcio<2,>=1.28.1 in /usr/local/lib/python3.10/dist-packages (from tfx) (1.64.1)\n",
            "Requirement already satisfied: keras-tuner!=1.4.0,!=1.4.1,<2,>=1.0.4 in /usr/local/lib/python3.10/dist-packages (from tfx) (1.4.7)\n",
            "Requirement already satisfied: kubernetes<13,>=10.0.1 in /usr/local/lib/python3.10/dist-packages (from tfx) (12.0.1)\n",
            "Requirement already satisfied: numpy<2,>=1.16 in /usr/local/lib/python3.10/dist-packages (from tfx) (1.26.4)\n",
            "Requirement already satisfied: pyarrow<11,>=10 in /usr/local/lib/python3.10/dist-packages (from tfx) (10.0.1)\n",
            "Requirement already satisfied: scipy<1.13 in /usr/local/lib/python3.10/dist-packages (from tfx) (1.12.0)\n",
            "Requirement already satisfied: pyyaml<7,>=6 in /usr/local/lib/python3.10/dist-packages (from tfx) (6.0.2)\n",
            "Requirement already satisfied: tensorflow<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tfx) (2.15.1)\n",
            "Requirement already satisfied: tensorflow-hub<0.16,>=0.15.0 in /usr/local/lib/python3.10/dist-packages (from tfx) (0.15.0)\n",
            "Requirement already satisfied: tensorflow-data-validation<1.16.0,>=1.15.1 in /usr/local/lib/python3.10/dist-packages (from tfx) (1.15.1)\n",
            "Requirement already satisfied: tensorflow-model-analysis<0.47.0,>=0.46.0 in /usr/local/lib/python3.10/dist-packages (from tfx) (0.46.0)\n",
            "Requirement already satisfied: tensorflow-serving-api<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tfx) (2.15.1)\n",
            "Requirement already satisfied: tensorflow-transform<1.16.0,>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from tfx) (1.15.0)\n",
            "Requirement already satisfied: tfx-bsl<1.16.0,>=1.15.1 in /usr/local/lib/python3.10/dist-packages (from tfx) (1.15.1)\n",
            "Requirement already satisfied: crcmod<2.0,>=1.7 in /usr/local/lib/python3.10/dist-packages (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tfx) (1.7)\n",
            "Requirement already satisfied: orjson<4,>=3.9.7 in /usr/local/lib/python3.10/dist-packages (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tfx) (3.10.7)\n",
            "Requirement already satisfied: dill<0.3.2,>=0.3.1.1 in /usr/local/lib/python3.10/dist-packages (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tfx) (0.3.1.1)\n",
            "Requirement already satisfied: cloudpickle~=2.2.1 in /usr/local/lib/python3.10/dist-packages (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tfx) (2.2.1)\n",
            "Requirement already satisfied: fastavro<2,>=0.23.6 in /usr/local/lib/python3.10/dist-packages (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tfx) (1.9.5)\n",
            "Requirement already satisfied: fasteners<1.0,>=0.3 in /usr/local/lib/python3.10/dist-packages (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tfx) (0.19)\n",
            "Requirement already satisfied: hdfs<3.0.0,>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tfx) (2.7.3)\n",
            "Requirement already satisfied: httplib2<0.23.0,>=0.8 in /usr/local/lib/python3.10/dist-packages (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tfx) (0.22.0)\n",
            "Requirement already satisfied: jsonschema<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tfx) (4.23.0)\n",
            "Requirement already satisfied: jsonpickle<4.0.0,>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tfx) (3.2.2)\n",
            "Requirement already satisfied: objsize<0.8.0,>=0.6.1 in /usr/local/lib/python3.10/dist-packages (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tfx) (0.7.0)\n",
            "Requirement already satisfied: pymongo<5.0.0,>=3.8.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tfx) (4.8.0)\n",
            "Requirement already satisfied: proto-plus<2,>=1.7.1 in /usr/local/lib/python3.10/dist-packages (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tfx) (1.24.0)\n",
            "Requirement already satisfied: pydot<2,>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tfx) (1.4.2)\n",
            "Requirement already satisfied: python-dateutil<3,>=2.8.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tfx) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2018.3 in /usr/local/lib/python3.10/dist-packages (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tfx) (2024.1)\n",
            "Requirement already satisfied: redis<6,>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tfx) (5.0.8)\n",
            "Requirement already satisfied: regex>=2020.6.8 in /usr/local/lib/python3.10/dist-packages (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tfx) (2024.5.15)\n",
            "Requirement already satisfied: requests!=2.32.*,<3.0.0,>=2.24.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tfx) (2.31.0)\n",
            "Requirement already satisfied: zstandard<1,>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tfx) (0.23.0)\n",
            "Requirement already satisfied: pyarrow-hotfix<1 in /usr/local/lib/python3.10/dist-packages (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tfx) (0.6)\n",
            "Requirement already satisfied: js2py<1,>=0.74 in /usr/local/lib/python3.10/dist-packages (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tfx) (0.74)\n",
            "Requirement already satisfied: cachetools<6,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (5.4.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (2.27.0)\n",
            "Requirement already satisfied: google-auth-httplib2<0.3.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (0.2.0)\n",
            "Requirement already satisfied: google-cloud-datastore<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (2.19.0)\n",
            "Requirement already satisfied: google-cloud-pubsub<3,>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (2.23.0)\n",
            "Requirement already satisfied: google-cloud-pubsublite<2,>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (1.11.1)\n",
            "Requirement already satisfied: google-cloud-storage<3,>=2.16.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (2.18.2)\n",
            "Requirement already satisfied: google-cloud-bigquery-storage<3,>=2.6.3 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (2.25.0)\n",
            "Requirement already satisfied: google-cloud-core<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (2.4.1)\n",
            "Requirement already satisfied: google-cloud-bigtable<3,>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (2.26.0)\n",
            "Requirement already satisfied: google-cloud-spanner<4,>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (3.48.0)\n",
            "Requirement already satisfied: google-cloud-dlp<4,>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (3.22.0)\n",
            "Requirement already satisfied: google-cloud-language<3,>=2.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (2.13.4)\n",
            "Requirement already satisfied: google-cloud-videointelligence<3,>=2.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (2.13.5)\n",
            "Requirement already satisfied: google-cloud-vision<4,>=2 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (3.7.4)\n",
            "Requirement already satisfied: google-cloud-recommendations-ai<0.11.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tfx) (0.10.12)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker<5,>=4.1->tfx) (1.16.0)\n",
            "Requirement already satisfied: websocket-client>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from docker<5,>=4.1->tfx) (1.8.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core<3->tfx) (1.63.2)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client<2,>=1.8->tfx) (3.0.1)\n",
            "Requirement already satisfied: oauth2client>=1.4.12 in /usr/local/lib/python3.10/dist-packages (from google-apitools<1,>=0.5->tfx) (4.1.3)\n",
            "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2,>=1.6.2->tfx) (1.12.5)\n",
            "Requirement already satisfied: shapely<3.0.0dev in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2,>=1.6.2->tfx) (2.0.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2,>=1.6.2->tfx) (2.8.2)\n",
            "Requirement already satisfied: docstring-parser<1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2,>=1.6.2->tfx) (0.16)\n",
            "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery<4,>=3->tfx) (2.7.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2<4,>=2.7.3->tfx) (2.1.5)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (from keras-tuner!=1.4.0,!=1.4.1,<2,>=1.0.4->tfx) (2.15.0)\n",
            "Requirement already satisfied: kt-legacy in /usr/local/lib/python3.10/dist-packages (from keras-tuner!=1.4.0,!=1.4.1,<2,>=1.0.4->tfx) (1.0.5)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.10/dist-packages (from kubernetes<13,>=10.0.1->tfx) (2024.7.4)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes<13,>=10.0.1->tfx) (71.0.4)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes<13,>=10.0.1->tfx) (1.3.1)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes<13,>=10.0.1->tfx) (2.0.7)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from portpicker<2,>=1.3.1->tfx) (5.9.5)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tfx) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tfx) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tfx) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tfx) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tfx) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tfx) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tfx) (0.3.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tfx) (3.3.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tfx) (2.4.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tfx) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tfx) (0.37.1)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tfx) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tfx) (2.15.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-data-validation<1.16.0,>=1.15.1->tfx) (1.4.2)\n",
            "Requirement already satisfied: pandas<2,>=1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-data-validation<1.16.0,>=1.15.1->tfx) (1.5.3)\n",
            "Requirement already satisfied: pyfarmhash<0.4,>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow-data-validation<1.16.0,>=1.15.1->tfx) (0.3.2)\n",
            "Requirement already satisfied: tensorflow-metadata<1.16,>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-data-validation<1.16.0,>=1.15.1->tfx) (1.15.0)\n",
            "Requirement already satisfied: ipython<8,>=7 in /usr/local/lib/python3.10/dist-packages (from tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (7.34.0)\n",
            "Requirement already satisfied: ipywidgets<8,>=7 in /usr/local/lib/python3.10/dist-packages (from tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (7.7.1)\n",
            "Requirement already satisfied: pillow>=9.4.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (9.4.0)\n",
            "Requirement already satisfied: rouge-score<2,>=0.1.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (0.1.2)\n",
            "Requirement already satisfied: sacrebleu<4,>=2.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (2.4.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.16,>=2.15.0->tfx) (0.44.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform<2,>=1.6.2->tfx) (1.48.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.18.0->apache-beam[gcp]<3,>=2.47->tfx) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.18.0->apache-beam[gcp]<3,>=2.47->tfx) (4.9)\n",
            "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigtable<3,>=2.19.0->apache-beam[gcp]<3,>=2.47->tfx) (0.13.1)\n",
            "Requirement already satisfied: overrides<8.0.0,>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-pubsublite<2,>=1.2.0->apache-beam[gcp]<3,>=2.47->tfx) (7.7.0)\n",
            "Requirement already satisfied: sqlparse>=0.4.4 in /usr/local/lib/python3.10/dist-packages (from google-cloud-spanner<4,>=3.0.0->apache-beam[gcp]<3,>=2.47->tfx) (0.5.1)\n",
            "Requirement already satisfied: grpc-interceptor>=0.15.4 in /usr/local/lib/python3.10/dist-packages (from google-cloud-spanner<4,>=3.0.0->apache-beam[gcp]<3,>=2.47->tfx) (0.15.4)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage<3,>=2.16.0->apache-beam[gcp]<3,>=2.47->tfx) (1.5.0)\n",
            "Requirement already satisfied: docopt in /usr/local/lib/python3.10/dist-packages (from hdfs<3.0.0,>=2.1.0->apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tfx) (0.6.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<0.23.0,>=0.8->apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tfx) (3.1.2)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython<8,>=7->tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (0.19.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython<8,>=7->tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython<8,>=7->tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython<8,>=7->tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython<8,>=7->tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (3.0.47)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython<8,>=7->tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython<8,>=7->tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython<8,>=7->tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython<8,>=7->tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (4.9.0)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (5.5.6)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (0.2.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (3.6.8)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (3.0.11)\n",
            "Requirement already satisfied: tzlocal>=1.2 in /usr/local/lib/python3.10/dist-packages (from js2py<1,>=0.74->apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tfx) (5.2)\n",
            "Requirement already satisfied: pyjsparser>=2.5.1 in /usr/local/lib/python3.10/dist-packages (from js2py<1,>=0.74->apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tfx) (2.7.1)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.0.0->apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tfx) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.0.0->apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tfx) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.0.0->apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tfx) (0.20.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.10/dist-packages (from oauth2client>=1.4.12->google-apitools<1,>=0.5->tfx) (0.6.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3->google-cloud-aiplatform<2,>=1.6.2->tfx) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3->google-cloud-aiplatform<2,>=1.6.2->tfx) (2.20.1)\n",
            "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from pymongo<5.0.0,>=3.8.0->apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tfx) (2.6.1)\n",
            "Requirement already satisfied: async-timeout>=4.0.3 in /usr/local/lib/python3.10/dist-packages (from redis<6,>=5.0.0->apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tfx) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests!=2.32.*,<3.0.0,>=2.24.0->apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tfx) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests!=2.32.*,<3.0.0,>=2.24.0->apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47->tfx) (3.7)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge-score<2,>=0.1.2->tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (3.8.1)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from sacrebleu<4,>=2.3->tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (2.10.1)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu<4,>=2.3->tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (0.9.0)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from sacrebleu<4,>=2.3->tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (0.4.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu<4,>=2.3->tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (4.9.4)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tfx) (1.2.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tfx) (3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tfx) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tfx) (3.0.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib->kubernetes<13,>=10.0.1->tfx) (3.2.2)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets<8,>=7->tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (6.1.12)\n",
            "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets<8,>=7->tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (6.3.3)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython<8,>=7->tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython<8,>=7->tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython<8,>=7->tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (0.2.13)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.10/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (6.5.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score<2,>=0.1.2->tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (4.66.5)\n",
            "Requirement already satisfied: pyzmq<25,>=17 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (24.0.1)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (23.1.0)\n",
            "Requirement already satisfied: jupyter-core>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (5.7.2)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (5.10.4)\n",
            "Requirement already satisfied: nbconvert>=5 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (6.5.4)\n",
            "Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (1.6.0)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (0.18.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (0.20.0)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (1.1.0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core>=4.6.1->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (4.2.2)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (0.2.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (4.12.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (6.1.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (0.7.1)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (0.4)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (0.3.0)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (0.8.4)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (0.10.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (1.5.1)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (1.3.0)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.10/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (2.20.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (21.2.0)\n",
            "Requirement already satisfied: jupyter-server<3,>=1.8 in /usr/local/lib/python3.10/dist-packages (from notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (1.24.0)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (1.17.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (2.6)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (0.5.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (2.22)\n",
            "Requirement already satisfied: anyio<4,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (3.7.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.47.0,>=0.46.0->tfx) (1.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U tfx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-08T09:40:18.583176Z",
          "iopub.status.busy": "2024-05-08T09:40:18.582900Z",
          "iopub.status.idle": "2024-05-08T09:40:24.609993Z",
          "shell.execute_reply": "2024-05-08T09:40:24.609125Z"
        },
        "id": "6jh7vKSRqPHb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "add6fba4-4346-42b4-ec4d-f31ed5e58772"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.15.1\n",
            "TFX version: 1.15.1\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print('TensorFlow version: {}'.format(tf.__version__))\n",
        "from tfx import v1 as tfx\n",
        "print('TFX version: {}'.format(tfx.__version__))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-08T09:40:24.614014Z",
          "iopub.status.busy": "2024-05-08T09:40:24.613213Z",
          "iopub.status.idle": "2024-05-08T09:40:24.617802Z",
          "shell.execute_reply": "2024-05-08T09:40:24.617143Z"
        },
        "id": "EcUseqJaE2XN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "PIPELINE_NAME = \"penguin-simple\"\n",
        "\n",
        "# Output directory to store artifacts generated from the pipeline.\n",
        "PIPELINE_ROOT = os.path.join('pipelines', PIPELINE_NAME)\n",
        "# Path to a SQLite DB file to use as an MLMD storage.\n",
        "METADATA_PATH = os.path.join('metadata', PIPELINE_NAME, 'metadata.db')\n",
        "# Output directory where created models from the pipeline will be exported.\n",
        "SERVING_MODEL_DIR = os.path.join('serving_model', PIPELINE_NAME)\n",
        "\n",
        "from absl import logging\n",
        "logging.set_verbosity(logging.INFO)  # Set default logging level."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-08T09:40:24.621155Z",
          "iopub.status.busy": "2024-05-08T09:40:24.620760Z",
          "iopub.status.idle": "2024-05-08T09:40:24.670423Z",
          "shell.execute_reply": "2024-05-08T09:40:24.669778Z"
        },
        "id": "4fxMs6u86acP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5667f04e-7614-4fa1-cbdf-4c788d78a030"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/tmp/tfx-datazyqqr44o/data.csv', <http.client.HTTPMessage at 0x7b581f7af430>)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "import urllib.request\n",
        "import tempfile\n",
        "\n",
        "DATA_ROOT = tempfile.mkdtemp(prefix='tfx-data')  # Create a temporary directory.\n",
        "_data_url = 'https://raw.githubusercontent.com/tensorflow/tfx/master/tfx/examples/penguin/data/labelled/penguins_processed.csv'\n",
        "_data_filepath = os.path.join(DATA_ROOT, \"data.csv\")\n",
        "urllib.request.urlretrieve(_data_url, _data_filepath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-08T09:40:24.673884Z",
          "iopub.status.busy": "2024-05-08T09:40:24.673642Z",
          "iopub.status.idle": "2024-05-08T09:40:24.810827Z",
          "shell.execute_reply": "2024-05-08T09:40:24.810015Z"
        },
        "id": "-eSz28UDSnlG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3659bf4d-e4d1-41c0-b901-32b586ea5c95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "species,culmen_length_mm,culmen_depth_mm,flipper_length_mm,body_mass_g\n",
            "0,0.2545454545454545,0.6666666666666666,0.15254237288135594,0.2916666666666667\n",
            "0,0.26909090909090905,0.5119047619047618,0.23728813559322035,0.3055555555555556\n",
            "0,0.29818181818181805,0.5833333333333334,0.3898305084745763,0.1527777777777778\n",
            "0,0.16727272727272732,0.7380952380952381,0.3559322033898305,0.20833333333333334\n",
            "0,0.26181818181818167,0.892857142857143,0.3050847457627119,0.2638888888888889\n",
            "0,0.24727272727272717,0.5595238095238096,0.15254237288135594,0.2569444444444444\n",
            "0,0.25818181818181823,0.773809523809524,0.3898305084745763,0.5486111111111112\n",
            "0,0.32727272727272727,0.5357142857142859,0.1694915254237288,0.1388888888888889\n",
            "0,0.23636363636363636,0.9642857142857142,0.3220338983050847,0.3055555555555556\n"
          ]
        }
      ],
      "source": [
        "!head {_data_filepath}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-08T09:40:24.815126Z",
          "iopub.status.busy": "2024-05-08T09:40:24.814834Z",
          "iopub.status.idle": "2024-05-08T09:40:24.818744Z",
          "shell.execute_reply": "2024-05-08T09:40:24.818056Z"
        },
        "id": "aES7Hv5QTDK3"
      },
      "outputs": [],
      "source": [
        "_trainer_module_file = 'penguin_trainer.py'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-08T09:40:24.822531Z",
          "iopub.status.busy": "2024-05-08T09:40:24.821916Z",
          "iopub.status.idle": "2024-05-08T09:40:24.828959Z",
          "shell.execute_reply": "2024-05-08T09:40:24.828287Z"
        },
        "id": "Gnc67uQNTDfW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16b5baf2-8926-496e-f436-077e4b169e8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting penguin_trainer.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile {_trainer_module_file}\n",
        "\n",
        "from typing import List\n",
        "from absl import logging\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow_transform.tf_metadata import schema_utils\n",
        "\n",
        "from tfx import v1 as tfx\n",
        "from tfx_bsl.public import tfxio\n",
        "from tensorflow_metadata.proto.v0 import schema_pb2\n",
        "\n",
        "_FEATURE_KEYS = [\n",
        "    'culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g'\n",
        "]\n",
        "_LABEL_KEY = 'species'\n",
        "\n",
        "_TRAIN_BATCH_SIZE = 20\n",
        "_EVAL_BATCH_SIZE = 10\n",
        "\n",
        "# Since we're not generating or creating a schema, we will instead create\n",
        "# a feature spec.  Since there are a fairly small number of features this is\n",
        "# manageable for this dataset.\n",
        "_FEATURE_SPEC = {\n",
        "    **{\n",
        "        feature: tf.io.FixedLenFeature(shape=[1], dtype=tf.float32)\n",
        "           for feature in _FEATURE_KEYS\n",
        "       },\n",
        "    _LABEL_KEY: tf.io.FixedLenFeature(shape=[1], dtype=tf.int64)\n",
        "}\n",
        "\n",
        "\n",
        "def _input_fn(file_pattern: List[str],\n",
        "              data_accessor: tfx.components.DataAccessor,\n",
        "              schema: schema_pb2.Schema,\n",
        "              batch_size: int = 200) -> tf.data.Dataset:\n",
        "  \"\"\"Generates features and label for training.\n",
        "\n",
        "  Args:\n",
        "    file_pattern: List of paths or patterns of input tfrecord files.\n",
        "    data_accessor: DataAccessor for converting input to RecordBatch.\n",
        "    schema: schema of the input data.\n",
        "    batch_size: representing the number of consecutive elements of returned\n",
        "      dataset to combine in a single batch\n",
        "\n",
        "  Returns:\n",
        "    A dataset that contains (features, indices) tuple where features is a\n",
        "      dictionary of Tensors, and indices is a single Tensor of label indices.\n",
        "  \"\"\"\n",
        "  return data_accessor.tf_dataset_factory(\n",
        "      file_pattern,\n",
        "      tfxio.TensorFlowDatasetOptions(\n",
        "          batch_size=batch_size, label_key=_LABEL_KEY),\n",
        "      schema=schema).repeat()\n",
        "\n",
        "\n",
        "def _build_keras_model() -> tf.keras.Model:\n",
        "  \"\"\"Creates a DNN Keras model for classifying penguin data.\n",
        "\n",
        "  Returns:\n",
        "    A Keras Model.\n",
        "  \"\"\"\n",
        "  # The model below is built with Functional API, please refer to\n",
        "  # https://www.tensorflow.org/guide/keras/overview for all API options.\n",
        "  inputs = [keras.layers.Input(shape=(1,), name=f) for f in _FEATURE_KEYS]\n",
        "  d = keras.layers.concatenate(inputs)\n",
        "  for _ in range(2):\n",
        "    d = keras.layers.Dense(8, activation='relu')(d)\n",
        "  outputs = keras.layers.Dense(3)(d)\n",
        "\n",
        "  model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "  model.compile(\n",
        "      optimizer=keras.optimizers.Adam(1e-2),\n",
        "      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "      metrics=[keras.metrics.SparseCategoricalAccuracy()])\n",
        "\n",
        "  model.summary(print_fn=logging.info)\n",
        "  return model\n",
        "\n",
        "\n",
        "# TFX Trainer will call this function.\n",
        "def run_fn(fn_args: tfx.components.FnArgs):\n",
        "  \"\"\"Train the model based on given args.\n",
        "\n",
        "  Args:\n",
        "    fn_args: Holds args used to train the model as name/value pairs.\n",
        "  \"\"\"\n",
        "\n",
        "  # This schema is usually either an output of SchemaGen or a manually-curated\n",
        "  # version provided by pipeline author. A schema can also derived from TFT\n",
        "  # graph if a Transform component is used. In the case when either is missing,\n",
        "  # `schema_from_feature_spec` could be used to generate schema from very simple\n",
        "  # feature_spec, but the schema returned would be very primitive.\n",
        "  schema = schema_utils.schema_from_feature_spec(_FEATURE_SPEC)\n",
        "\n",
        "  train_dataset = _input_fn(\n",
        "      fn_args.train_files,\n",
        "      fn_args.data_accessor,\n",
        "      schema,\n",
        "      batch_size=_TRAIN_BATCH_SIZE)\n",
        "  eval_dataset = _input_fn(\n",
        "      fn_args.eval_files,\n",
        "      fn_args.data_accessor,\n",
        "      schema,\n",
        "      batch_size=_EVAL_BATCH_SIZE)\n",
        "\n",
        "  model = _build_keras_model()\n",
        "  model.fit(\n",
        "      train_dataset,\n",
        "      steps_per_epoch=fn_args.train_steps,\n",
        "      validation_data=eval_dataset,\n",
        "      validation_steps=fn_args.eval_steps)\n",
        "\n",
        "  # The result of the training should be saved in `fn_args.serving_model_dir`\n",
        "  # directory.\n",
        "  model.save(fn_args.serving_model_dir, save_format='tf')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-08T09:40:24.832718Z",
          "iopub.status.busy": "2024-05-08T09:40:24.832106Z",
          "iopub.status.idle": "2024-05-08T09:40:24.838381Z",
          "shell.execute_reply": "2024-05-08T09:40:24.837728Z"
        },
        "id": "M49yYVNBTPd4"
      },
      "outputs": [],
      "source": [
        "def _create_pipeline(pipeline_name: str, pipeline_root: str, data_root: str,\n",
        "                     module_file: str, serving_model_dir: str,\n",
        "                     metadata_path: str) -> tfx.dsl.Pipeline:\n",
        "  \"\"\"Creates a three component penguin pipeline with TFX.\"\"\"\n",
        "  # Brings data into the pipeline.\n",
        "  example_gen = tfx.components.CsvExampleGen(input_base=data_root)\n",
        "\n",
        "  # Uses user-provided Python function that trains a model.\n",
        "  trainer = tfx.components.Trainer(\n",
        "      module_file=module_file,\n",
        "      examples=example_gen.outputs['examples'],\n",
        "      train_args=tfx.proto.TrainArgs(num_steps=100),\n",
        "      eval_args=tfx.proto.EvalArgs(num_steps=5))\n",
        "\n",
        "  # Pushes the model to a filesystem destination.\n",
        "  pusher = tfx.components.Pusher(\n",
        "      model=trainer.outputs['model'],\n",
        "      push_destination=tfx.proto.PushDestination(\n",
        "          filesystem=tfx.proto.PushDestination.Filesystem(\n",
        "              base_directory=serving_model_dir)))\n",
        "\n",
        "  # Following three components will be included in the pipeline.\n",
        "  components = [\n",
        "      example_gen,\n",
        "      trainer,\n",
        "      pusher,\n",
        "  ]\n",
        "\n",
        "  return tfx.dsl.Pipeline(\n",
        "      pipeline_name=pipeline_name,\n",
        "      pipeline_root=pipeline_root,\n",
        "      metadata_connection_config=tfx.orchestration.metadata\n",
        "      .sqlite_metadata_connection_config(metadata_path),\n",
        "      components=components)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-08T09:40:24.841880Z",
          "iopub.status.busy": "2024-05-08T09:40:24.841619Z",
          "iopub.status.idle": "2024-05-08T09:40:35.669308Z",
          "shell.execute_reply": "2024-05-08T09:40:35.668540Z"
        },
        "id": "fAtfOZTYWJu-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "da7fae76-2066-4e38-a7ef-5c8986e49a09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:absl:Generating ephemeral wheel package for '/content/penguin_trainer.py' (including modules: ['penguin_trainer']).\n",
            "INFO:absl:User module package has hash fingerprint version a7e2e8dccbb913b74904edeec5549d868a2ea392bcd84fbc1965aba698dce3fc.\n",
            "INFO:absl:Executing: ['/usr/bin/python3', '/tmp/tmpzneuk8ai/_tfx_generated_setup.py', 'bdist_wheel', '--bdist-dir', '/tmp/tmp8hirtfhm', '--dist-dir', '/tmp/tmpb5j2lqip']\n",
            "INFO:absl:Successfully built user code wheel distribution at 'pipelines/penguin-simple/_wheels/tfx_user_code_Trainer-0.0+a7e2e8dccbb913b74904edeec5549d868a2ea392bcd84fbc1965aba698dce3fc-py3-none-any.whl'; target user module is 'penguin_trainer'.\n",
            "INFO:absl:Full user module path is 'penguin_trainer@pipelines/penguin-simple/_wheels/tfx_user_code_Trainer-0.0+a7e2e8dccbb913b74904edeec5549d868a2ea392bcd84fbc1965aba698dce3fc-py3-none-any.whl'\n",
            "INFO:absl:Using deployment config:\n",
            " executor_specs {\n",
            "  key: \"CsvExampleGen\"\n",
            "  value {\n",
            "    beam_executable_spec {\n",
            "      python_executor_spec {\n",
            "        class_path: \"tfx.components.example_gen.csv_example_gen.executor.Executor\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "executor_specs {\n",
            "  key: \"Pusher\"\n",
            "  value {\n",
            "    python_class_executable_spec {\n",
            "      class_path: \"tfx.components.pusher.executor.Executor\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "executor_specs {\n",
            "  key: \"Trainer\"\n",
            "  value {\n",
            "    python_class_executable_spec {\n",
            "      class_path: \"tfx.components.trainer.executor.GenericExecutor\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "custom_driver_specs {\n",
            "  key: \"CsvExampleGen\"\n",
            "  value {\n",
            "    python_class_executable_spec {\n",
            "      class_path: \"tfx.components.example_gen.driver.FileBasedDriver\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "metadata_connection_config {\n",
            "  database_connection_config {\n",
            "    sqlite {\n",
            "      filename_uri: \"metadata/penguin-simple/metadata.db\"\n",
            "      connection_mode: READWRITE_OPENCREATE\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n",
            "INFO:absl:Using connection config:\n",
            " sqlite {\n",
            "  filename_uri: \"metadata/penguin-simple/metadata.db\"\n",
            "  connection_mode: READWRITE_OPENCREATE\n",
            "}\n",
            "\n",
            "INFO:absl:Component CsvExampleGen is running.\n",
            "INFO:absl:Running launcher for node_info {\n",
            "  type {\n",
            "    name: \"tfx.components.example_gen.csv_example_gen.component.CsvExampleGen\"\n",
            "  }\n",
            "  id: \"CsvExampleGen\"\n",
            "}\n",
            "contexts {\n",
            "  contexts {\n",
            "    type {\n",
            "      name: \"pipeline\"\n",
            "    }\n",
            "    name {\n",
            "      field_value {\n",
            "        string_value: \"penguin-simple\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  contexts {\n",
            "    type {\n",
            "      name: \"pipeline_run\"\n",
            "    }\n",
            "    name {\n",
            "      field_value {\n",
            "        string_value: \"2024-08-22T06:20:44.717237\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  contexts {\n",
            "    type {\n",
            "      name: \"node\"\n",
            "    }\n",
            "    name {\n",
            "      field_value {\n",
            "        string_value: \"penguin-simple.CsvExampleGen\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "outputs {\n",
            "  outputs {\n",
            "    key: \"examples\"\n",
            "    value {\n",
            "      artifact_spec {\n",
            "        type {\n",
            "          name: \"Examples\"\n",
            "          properties {\n",
            "            key: \"span\"\n",
            "            value: INT\n",
            "          }\n",
            "          properties {\n",
            "            key: \"split_names\"\n",
            "            value: STRING\n",
            "          }\n",
            "          properties {\n",
            "            key: \"version\"\n",
            "            value: INT\n",
            "          }\n",
            "          base_type: DATASET\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "parameters {\n",
            "  parameters {\n",
            "    key: \"input_base\"\n",
            "    value {\n",
            "      field_value {\n",
            "        string_value: \"/tmp/tfx-datazyqqr44o\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  parameters {\n",
            "    key: \"input_config\"\n",
            "    value {\n",
            "      field_value {\n",
            "        string_value: \"{\\n  \\\"splits\\\": [\\n    {\\n      \\\"name\\\": \\\"single_split\\\",\\n      \\\"pattern\\\": \\\"*\\\"\\n    }\\n  ]\\n}\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  parameters {\n",
            "    key: \"output_config\"\n",
            "    value {\n",
            "      field_value {\n",
            "        string_value: \"{\\n  \\\"split_config\\\": {\\n    \\\"splits\\\": [\\n      {\\n        \\\"hash_buckets\\\": 2,\\n        \\\"name\\\": \\\"train\\\"\\n      },\\n      {\\n        \\\"hash_buckets\\\": 1,\\n        \\\"name\\\": \\\"eval\\\"\\n      }\\n    ]\\n  }\\n}\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  parameters {\n",
            "    key: \"output_data_format\"\n",
            "    value {\n",
            "      field_value {\n",
            "        int_value: 6\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  parameters {\n",
            "    key: \"output_file_format\"\n",
            "    value {\n",
            "      field_value {\n",
            "        int_value: 5\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "downstream_nodes: \"Trainer\"\n",
            "execution_options {\n",
            "  caching_options {\n",
            "  }\n",
            "}\n",
            "\n",
            "INFO:absl:MetadataStore with DB connection initialized\n",
            "INFO:absl:[CsvExampleGen] Resolved inputs: ({},)\n",
            "INFO:absl:select span and version = (0, None)\n",
            "INFO:absl:latest span and version = (0, None)\n",
            "INFO:absl:MetadataStore with DB connection initialized\n",
            "INFO:absl:Going to run a new execution 4\n",
            "INFO:absl:Going to run a new execution: ExecutionInfo(execution_id=4, input_dict={}, output_dict=defaultdict(<class 'list'>, {'examples': [Artifact(artifact: uri: \"pipelines/penguin-simple/CsvExampleGen/examples/4\"\n",
            "custom_properties {\n",
            "  key: \"input_fingerprint\"\n",
            "  value {\n",
            "    string_value: \"split:single_split,num_files:1,total_bytes:25648,xor_checksum:1724307643,sum_checksum:1724307643\"\n",
            "  }\n",
            "}\n",
            "custom_properties {\n",
            "  key: \"span\"\n",
            "  value {\n",
            "    int_value: 0\n",
            "  }\n",
            "}\n",
            ", artifact_type: name: \"Examples\"\n",
            "properties {\n",
            "  key: \"span\"\n",
            "  value: INT\n",
            "}\n",
            "properties {\n",
            "  key: \"split_names\"\n",
            "  value: STRING\n",
            "}\n",
            "properties {\n",
            "  key: \"version\"\n",
            "  value: INT\n",
            "}\n",
            "base_type: DATASET\n",
            ")]}), exec_properties={'output_config': '{\\n  \"split_config\": {\\n    \"splits\": [\\n      {\\n        \"hash_buckets\": 2,\\n        \"name\": \"train\"\\n      },\\n      {\\n        \"hash_buckets\": 1,\\n        \"name\": \"eval\"\\n      }\\n    ]\\n  }\\n}', 'output_file_format': 5, 'input_base': '/tmp/tfx-datazyqqr44o', 'output_data_format': 6, 'input_config': '{\\n  \"splits\": [\\n    {\\n      \"name\": \"single_split\",\\n      \"pattern\": \"*\"\\n    }\\n  ]\\n}', 'span': 0, 'version': None, 'input_fingerprint': 'split:single_split,num_files:1,total_bytes:25648,xor_checksum:1724307643,sum_checksum:1724307643'}, execution_output_uri='pipelines/penguin-simple/CsvExampleGen/.system/executor_execution/4/executor_output.pb', stateful_working_dir='pipelines/penguin-simple/CsvExampleGen/.system/stateful_working_dir/9b907905-468e-40d9-94f5-e0f22a565160', tmp_dir='pipelines/penguin-simple/CsvExampleGen/.system/executor_execution/4/.temp/', pipeline_node=node_info {\n",
            "  type {\n",
            "    name: \"tfx.components.example_gen.csv_example_gen.component.CsvExampleGen\"\n",
            "  }\n",
            "  id: \"CsvExampleGen\"\n",
            "}\n",
            "contexts {\n",
            "  contexts {\n",
            "    type {\n",
            "      name: \"pipeline\"\n",
            "    }\n",
            "    name {\n",
            "      field_value {\n",
            "        string_value: \"penguin-simple\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  contexts {\n",
            "    type {\n",
            "      name: \"pipeline_run\"\n",
            "    }\n",
            "    name {\n",
            "      field_value {\n",
            "        string_value: \"2024-08-22T06:20:44.717237\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  contexts {\n",
            "    type {\n",
            "      name: \"node\"\n",
            "    }\n",
            "    name {\n",
            "      field_value {\n",
            "        string_value: \"penguin-simple.CsvExampleGen\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "outputs {\n",
            "  outputs {\n",
            "    key: \"examples\"\n",
            "    value {\n",
            "      artifact_spec {\n",
            "        type {\n",
            "          name: \"Examples\"\n",
            "          properties {\n",
            "            key: \"span\"\n",
            "            value: INT\n",
            "          }\n",
            "          properties {\n",
            "            key: \"split_names\"\n",
            "            value: STRING\n",
            "          }\n",
            "          properties {\n",
            "            key: \"version\"\n",
            "            value: INT\n",
            "          }\n",
            "          base_type: DATASET\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "parameters {\n",
            "  parameters {\n",
            "    key: \"input_base\"\n",
            "    value {\n",
            "      field_value {\n",
            "        string_value: \"/tmp/tfx-datazyqqr44o\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  parameters {\n",
            "    key: \"input_config\"\n",
            "    value {\n",
            "      field_value {\n",
            "        string_value: \"{\\n  \\\"splits\\\": [\\n    {\\n      \\\"name\\\": \\\"single_split\\\",\\n      \\\"pattern\\\": \\\"*\\\"\\n    }\\n  ]\\n}\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  parameters {\n",
            "    key: \"output_config\"\n",
            "    value {\n",
            "      field_value {\n",
            "        string_value: \"{\\n  \\\"split_config\\\": {\\n    \\\"splits\\\": [\\n      {\\n        \\\"hash_buckets\\\": 2,\\n        \\\"name\\\": \\\"train\\\"\\n      },\\n      {\\n        \\\"hash_buckets\\\": 1,\\n        \\\"name\\\": \\\"eval\\\"\\n      }\\n    ]\\n  }\\n}\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  parameters {\n",
            "    key: \"output_data_format\"\n",
            "    value {\n",
            "      field_value {\n",
            "        int_value: 6\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  parameters {\n",
            "    key: \"output_file_format\"\n",
            "    value {\n",
            "      field_value {\n",
            "        int_value: 5\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "downstream_nodes: \"Trainer\"\n",
            "execution_options {\n",
            "  caching_options {\n",
            "  }\n",
            "}\n",
            ", pipeline_info=id: \"penguin-simple\"\n",
            ", pipeline_run_id='2024-08-22T06:20:44.717237', top_level_pipeline_run_id=None, frontend_url=None)\n",
            "INFO:absl:Generating examples.\n",
            "WARNING:apache_beam.runners.interactive.interactive_environment:Dependencies required for Interactive Beam PCollection visualization are not available, please use: `pip install apache-beam[interactive]` to install necessary dependencies to enable all data visualization features.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        if (typeof window.interactive_beam_jquery == 'undefined') {\n",
              "          var jqueryScript = document.createElement('script');\n",
              "          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n",
              "          jqueryScript.type = 'text/javascript';\n",
              "          jqueryScript.onload = function() {\n",
              "            var datatableScript = document.createElement('script');\n",
              "            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n",
              "            datatableScript.type = 'text/javascript';\n",
              "            datatableScript.onload = function() {\n",
              "              window.interactive_beam_jquery = jQuery.noConflict(true);\n",
              "              window.interactive_beam_jquery(document).ready(function($){\n",
              "                \n",
              "              });\n",
              "            }\n",
              "            document.head.appendChild(datatableScript);\n",
              "          };\n",
              "          document.head.appendChild(jqueryScript);\n",
              "        } else {\n",
              "          window.interactive_beam_jquery(document).ready(function($){\n",
              "            \n",
              "          });\n",
              "        }"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:absl:Processing input csv data /tmp/tfx-datazyqqr44o/* to TFExample.\n",
            "WARNING:apache_beam.io.tfrecordio:Couldn't find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\n",
            "INFO:absl:Examples generated.\n",
            "INFO:absl:Value type <class 'NoneType'> of key version in exec_properties is not supported, going to drop it\n",
            "INFO:absl:Value type <class 'list'> of key _beam_pipeline_args in exec_properties is not supported, going to drop it\n",
            "INFO:absl:Cleaning up stateless execution info.\n",
            "INFO:absl:Execution 4 succeeded.\n",
            "INFO:absl:Cleaning up stateful execution info.\n",
            "INFO:absl:Deleted stateful_working_dir pipelines/penguin-simple/CsvExampleGen/.system/stateful_working_dir/9b907905-468e-40d9-94f5-e0f22a565160\n",
            "INFO:absl:Publishing output artifacts defaultdict(<class 'list'>, {'examples': [Artifact(artifact: uri: \"pipelines/penguin-simple/CsvExampleGen/examples/4\"\n",
            "custom_properties {\n",
            "  key: \"input_fingerprint\"\n",
            "  value {\n",
            "    string_value: \"split:single_split,num_files:1,total_bytes:25648,xor_checksum:1724307643,sum_checksum:1724307643\"\n",
            "  }\n",
            "}\n",
            "custom_properties {\n",
            "  key: \"span\"\n",
            "  value {\n",
            "    int_value: 0\n",
            "  }\n",
            "}\n",
            ", artifact_type: name: \"Examples\"\n",
            "properties {\n",
            "  key: \"span\"\n",
            "  value: INT\n",
            "}\n",
            "properties {\n",
            "  key: \"split_names\"\n",
            "  value: STRING\n",
            "}\n",
            "properties {\n",
            "  key: \"version\"\n",
            "  value: INT\n",
            "}\n",
            "base_type: DATASET\n",
            ")]}) for execution 4\n",
            "INFO:absl:MetadataStore with DB connection initialized\n",
            "INFO:absl:Component CsvExampleGen is finished.\n",
            "INFO:absl:Component Trainer is running.\n",
            "INFO:absl:Running launcher for node_info {\n",
            "  type {\n",
            "    name: \"tfx.components.trainer.component.Trainer\"\n",
            "    base_type: TRAIN\n",
            "  }\n",
            "  id: \"Trainer\"\n",
            "}\n",
            "contexts {\n",
            "  contexts {\n",
            "    type {\n",
            "      name: \"pipeline\"\n",
            "    }\n",
            "    name {\n",
            "      field_value {\n",
            "        string_value: \"penguin-simple\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  contexts {\n",
            "    type {\n",
            "      name: \"pipeline_run\"\n",
            "    }\n",
            "    name {\n",
            "      field_value {\n",
            "        string_value: \"2024-08-22T06:20:44.717237\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  contexts {\n",
            "    type {\n",
            "      name: \"node\"\n",
            "    }\n",
            "    name {\n",
            "      field_value {\n",
            "        string_value: \"penguin-simple.Trainer\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "inputs {\n",
            "  inputs {\n",
            "    key: \"examples\"\n",
            "    value {\n",
            "      channels {\n",
            "        producer_node_query {\n",
            "          id: \"CsvExampleGen\"\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"pipeline\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"penguin-simple\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"pipeline_run\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"2024-08-22T06:20:44.717237\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"node\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"penguin-simple.CsvExampleGen\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        artifact_query {\n",
            "          type {\n",
            "            name: \"Examples\"\n",
            "            base_type: DATASET\n",
            "          }\n",
            "        }\n",
            "        output_key: \"examples\"\n",
            "      }\n",
            "      min_count: 1\n",
            "    }\n",
            "  }\n",
            "}\n",
            "outputs {\n",
            "  outputs {\n",
            "    key: \"model\"\n",
            "    value {\n",
            "      artifact_spec {\n",
            "        type {\n",
            "          name: \"Model\"\n",
            "          base_type: MODEL\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  outputs {\n",
            "    key: \"model_run\"\n",
            "    value {\n",
            "      artifact_spec {\n",
            "        type {\n",
            "          name: \"ModelRun\"\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "parameters {\n",
            "  parameters {\n",
            "    key: \"custom_config\"\n",
            "    value {\n",
            "      field_value {\n",
            "        string_value: \"null\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  parameters {\n",
            "    key: \"eval_args\"\n",
            "    value {\n",
            "      field_value {\n",
            "        string_value: \"{\\n  \\\"num_steps\\\": 5\\n}\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  parameters {\n",
            "    key: \"module_path\"\n",
            "    value {\n",
            "      field_value {\n",
            "        string_value: \"penguin_trainer@pipelines/penguin-simple/_wheels/tfx_user_code_Trainer-0.0+a7e2e8dccbb913b74904edeec5549d868a2ea392bcd84fbc1965aba698dce3fc-py3-none-any.whl\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  parameters {\n",
            "    key: \"train_args\"\n",
            "    value {\n",
            "      field_value {\n",
            "        string_value: \"{\\n  \\\"num_steps\\\": 100\\n}\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "upstream_nodes: \"CsvExampleGen\"\n",
            "downstream_nodes: \"Pusher\"\n",
            "execution_options {\n",
            "  caching_options {\n",
            "  }\n",
            "}\n",
            "\n",
            "INFO:absl:MetadataStore with DB connection initialized\n",
            "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
            "INFO:absl:[Trainer] Resolved inputs: ({'examples': [Artifact(artifact: id: 5\n",
            "type_id: 15\n",
            "uri: \"pipelines/penguin-simple/CsvExampleGen/examples/4\"\n",
            "properties {\n",
            "  key: \"split_names\"\n",
            "  value {\n",
            "    string_value: \"[\\\"train\\\", \\\"eval\\\"]\"\n",
            "  }\n",
            "}\n",
            "custom_properties {\n",
            "  key: \"file_format\"\n",
            "  value {\n",
            "    string_value: \"tfrecords_gzip\"\n",
            "  }\n",
            "}\n",
            "custom_properties {\n",
            "  key: \"input_fingerprint\"\n",
            "  value {\n",
            "    string_value: \"split:single_split,num_files:1,total_bytes:25648,xor_checksum:1724307643,sum_checksum:1724307643\"\n",
            "  }\n",
            "}\n",
            "custom_properties {\n",
            "  key: \"is_external\"\n",
            "  value {\n",
            "    int_value: 0\n",
            "  }\n",
            "}\n",
            "custom_properties {\n",
            "  key: \"payload_format\"\n",
            "  value {\n",
            "    string_value: \"FORMAT_TF_EXAMPLE\"\n",
            "  }\n",
            "}\n",
            "custom_properties {\n",
            "  key: \"span\"\n",
            "  value {\n",
            "    int_value: 0\n",
            "  }\n",
            "}\n",
            "custom_properties {\n",
            "  key: \"tfx_version\"\n",
            "  value {\n",
            "    string_value: \"1.15.1\"\n",
            "  }\n",
            "}\n",
            "state: LIVE\n",
            "type: \"Examples\"\n",
            "create_time_since_epoch: 1724307649463\n",
            "last_update_time_since_epoch: 1724307649463\n",
            ", artifact_type: id: 15\n",
            "name: \"Examples\"\n",
            "properties {\n",
            "  key: \"span\"\n",
            "  value: INT\n",
            "}\n",
            "properties {\n",
            "  key: \"split_names\"\n",
            "  value: STRING\n",
            "}\n",
            "properties {\n",
            "  key: \"version\"\n",
            "  value: INT\n",
            "}\n",
            "base_type: DATASET\n",
            ")]},)\n",
            "INFO:absl:MetadataStore with DB connection initialized\n",
            "INFO:absl:Going to run a new execution 5\n",
            "INFO:absl:Going to run a new execution: ExecutionInfo(execution_id=5, input_dict={'examples': [Artifact(artifact: id: 5\n",
            "type_id: 15\n",
            "uri: \"pipelines/penguin-simple/CsvExampleGen/examples/4\"\n",
            "properties {\n",
            "  key: \"split_names\"\n",
            "  value {\n",
            "    string_value: \"[\\\"train\\\", \\\"eval\\\"]\"\n",
            "  }\n",
            "}\n",
            "custom_properties {\n",
            "  key: \"file_format\"\n",
            "  value {\n",
            "    string_value: \"tfrecords_gzip\"\n",
            "  }\n",
            "}\n",
            "custom_properties {\n",
            "  key: \"input_fingerprint\"\n",
            "  value {\n",
            "    string_value: \"split:single_split,num_files:1,total_bytes:25648,xor_checksum:1724307643,sum_checksum:1724307643\"\n",
            "  }\n",
            "}\n",
            "custom_properties {\n",
            "  key: \"is_external\"\n",
            "  value {\n",
            "    int_value: 0\n",
            "  }\n",
            "}\n",
            "custom_properties {\n",
            "  key: \"payload_format\"\n",
            "  value {\n",
            "    string_value: \"FORMAT_TF_EXAMPLE\"\n",
            "  }\n",
            "}\n",
            "custom_properties {\n",
            "  key: \"span\"\n",
            "  value {\n",
            "    int_value: 0\n",
            "  }\n",
            "}\n",
            "custom_properties {\n",
            "  key: \"tfx_version\"\n",
            "  value {\n",
            "    string_value: \"1.15.1\"\n",
            "  }\n",
            "}\n",
            "state: LIVE\n",
            "type: \"Examples\"\n",
            "create_time_since_epoch: 1724307649463\n",
            "last_update_time_since_epoch: 1724307649463\n",
            ", artifact_type: id: 15\n",
            "name: \"Examples\"\n",
            "properties {\n",
            "  key: \"span\"\n",
            "  value: INT\n",
            "}\n",
            "properties {\n",
            "  key: \"split_names\"\n",
            "  value: STRING\n",
            "}\n",
            "properties {\n",
            "  key: \"version\"\n",
            "  value: INT\n",
            "}\n",
            "base_type: DATASET\n",
            ")]}, output_dict=defaultdict(<class 'list'>, {'model_run': [Artifact(artifact: uri: \"pipelines/penguin-simple/Trainer/model_run/5\"\n",
            ", artifact_type: name: \"ModelRun\"\n",
            ")], 'model': [Artifact(artifact: uri: \"pipelines/penguin-simple/Trainer/model/5\"\n",
            ", artifact_type: name: \"Model\"\n",
            "base_type: MODEL\n",
            ")]}), exec_properties={'custom_config': 'null', 'eval_args': '{\\n  \"num_steps\": 5\\n}', 'train_args': '{\\n  \"num_steps\": 100\\n}', 'module_path': 'penguin_trainer@pipelines/penguin-simple/_wheels/tfx_user_code_Trainer-0.0+a7e2e8dccbb913b74904edeec5549d868a2ea392bcd84fbc1965aba698dce3fc-py3-none-any.whl'}, execution_output_uri='pipelines/penguin-simple/Trainer/.system/executor_execution/5/executor_output.pb', stateful_working_dir='pipelines/penguin-simple/Trainer/.system/stateful_working_dir/4c2c0883-0380-4e7e-9b19-b641f44ad4a9', tmp_dir='pipelines/penguin-simple/Trainer/.system/executor_execution/5/.temp/', pipeline_node=node_info {\n",
            "  type {\n",
            "    name: \"tfx.components.trainer.component.Trainer\"\n",
            "    base_type: TRAIN\n",
            "  }\n",
            "  id: \"Trainer\"\n",
            "}\n",
            "contexts {\n",
            "  contexts {\n",
            "    type {\n",
            "      name: \"pipeline\"\n",
            "    }\n",
            "    name {\n",
            "      field_value {\n",
            "        string_value: \"penguin-simple\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  contexts {\n",
            "    type {\n",
            "      name: \"pipeline_run\"\n",
            "    }\n",
            "    name {\n",
            "      field_value {\n",
            "        string_value: \"2024-08-22T06:20:44.717237\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  contexts {\n",
            "    type {\n",
            "      name: \"node\"\n",
            "    }\n",
            "    name {\n",
            "      field_value {\n",
            "        string_value: \"penguin-simple.Trainer\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "inputs {\n",
            "  inputs {\n",
            "    key: \"examples\"\n",
            "    value {\n",
            "      channels {\n",
            "        producer_node_query {\n",
            "          id: \"CsvExampleGen\"\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"pipeline\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"penguin-simple\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"pipeline_run\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"2024-08-22T06:20:44.717237\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"node\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"penguin-simple.CsvExampleGen\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        artifact_query {\n",
            "          type {\n",
            "            name: \"Examples\"\n",
            "            base_type: DATASET\n",
            "          }\n",
            "        }\n",
            "        output_key: \"examples\"\n",
            "      }\n",
            "      min_count: 1\n",
            "    }\n",
            "  }\n",
            "}\n",
            "outputs {\n",
            "  outputs {\n",
            "    key: \"model\"\n",
            "    value {\n",
            "      artifact_spec {\n",
            "        type {\n",
            "          name: \"Model\"\n",
            "          base_type: MODEL\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  outputs {\n",
            "    key: \"model_run\"\n",
            "    value {\n",
            "      artifact_spec {\n",
            "        type {\n",
            "          name: \"ModelRun\"\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "parameters {\n",
            "  parameters {\n",
            "    key: \"custom_config\"\n",
            "    value {\n",
            "      field_value {\n",
            "        string_value: \"null\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  parameters {\n",
            "    key: \"eval_args\"\n",
            "    value {\n",
            "      field_value {\n",
            "        string_value: \"{\\n  \\\"num_steps\\\": 5\\n}\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  parameters {\n",
            "    key: \"module_path\"\n",
            "    value {\n",
            "      field_value {\n",
            "        string_value: \"penguin_trainer@pipelines/penguin-simple/_wheels/tfx_user_code_Trainer-0.0+a7e2e8dccbb913b74904edeec5549d868a2ea392bcd84fbc1965aba698dce3fc-py3-none-any.whl\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  parameters {\n",
            "    key: \"train_args\"\n",
            "    value {\n",
            "      field_value {\n",
            "        string_value: \"{\\n  \\\"num_steps\\\": 100\\n}\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "upstream_nodes: \"CsvExampleGen\"\n",
            "downstream_nodes: \"Pusher\"\n",
            "execution_options {\n",
            "  caching_options {\n",
            "  }\n",
            "}\n",
            ", pipeline_info=id: \"penguin-simple\"\n",
            ", pipeline_run_id='2024-08-22T06:20:44.717237', top_level_pipeline_run_id=None, frontend_url=None)\n",
            "INFO:absl:Train on the 'train' split when train_args.splits is not set.\n",
            "INFO:absl:Evaluate on the 'eval' split when eval_args.splits is not set.\n",
            "INFO:absl:udf_utils.get_fn {'custom_config': 'null', 'eval_args': '{\\n  \"num_steps\": 5\\n}', 'train_args': '{\\n  \"num_steps\": 100\\n}', 'module_path': 'penguin_trainer@pipelines/penguin-simple/_wheels/tfx_user_code_Trainer-0.0+a7e2e8dccbb913b74904edeec5549d868a2ea392bcd84fbc1965aba698dce3fc-py3-none-any.whl'} 'run_fn'\n",
            "INFO:absl:Installing 'pipelines/penguin-simple/_wheels/tfx_user_code_Trainer-0.0+a7e2e8dccbb913b74904edeec5549d868a2ea392bcd84fbc1965aba698dce3fc-py3-none-any.whl' to a temporary directory.\n",
            "INFO:absl:Executing: ['/usr/bin/python3', '-m', 'pip', 'install', '--target', '/tmp/tmpzquno148', 'pipelines/penguin-simple/_wheels/tfx_user_code_Trainer-0.0+a7e2e8dccbb913b74904edeec5549d868a2ea392bcd84fbc1965aba698dce3fc-py3-none-any.whl']\n",
            "INFO:absl:Successfully installed 'pipelines/penguin-simple/_wheels/tfx_user_code_Trainer-0.0+a7e2e8dccbb913b74904edeec5549d868a2ea392bcd84fbc1965aba698dce3fc-py3-none-any.whl'.\n",
            "INFO:absl:Training model.\n",
            "INFO:absl:Feature body_mass_g has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "INFO:absl:Feature culmen_depth_mm has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "INFO:absl:Feature culmen_length_mm has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "INFO:absl:Feature flipper_length_mm has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "INFO:absl:Feature species has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tfx_bsl/tfxio/tf_example_record.py:343: parse_example_dataset (from tensorflow.python.data.experimental.ops.parsing_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.map(tf.io.parse_example(...))` instead.\n",
            "INFO:absl:Feature body_mass_g has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "INFO:absl:Feature culmen_depth_mm has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "INFO:absl:Feature culmen_length_mm has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "INFO:absl:Feature flipper_length_mm has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "INFO:absl:Feature species has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "INFO:absl:Feature body_mass_g has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "INFO:absl:Feature culmen_depth_mm has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "INFO:absl:Feature culmen_length_mm has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "INFO:absl:Feature flipper_length_mm has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "INFO:absl:Feature species has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "INFO:absl:Feature body_mass_g has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "INFO:absl:Feature culmen_depth_mm has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "INFO:absl:Feature culmen_length_mm has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "INFO:absl:Feature flipper_length_mm has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "INFO:absl:Feature species has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "INFO:absl:Model: \"model\"\n",
            "INFO:absl:__________________________________________________________________________________________________\n",
            "INFO:absl: Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "INFO:absl:==================================================================================================\n",
            "INFO:absl: culmen_length_mm (InputLay  [(None, 1)]                  0         []                            \n",
            "INFO:absl: er)                                                                                              \n",
            "INFO:absl:                                                                                                  \n",
            "INFO:absl: culmen_depth_mm (InputLaye  [(None, 1)]                  0         []                            \n",
            "INFO:absl: r)                                                                                               \n",
            "INFO:absl:                                                                                                  \n",
            "INFO:absl: flipper_length_mm (InputLa  [(None, 1)]                  0         []                            \n",
            "INFO:absl: yer)                                                                                             \n",
            "INFO:absl:                                                                                                  \n",
            "INFO:absl: body_mass_g (InputLayer)    [(None, 1)]                  0         []                            \n",
            "INFO:absl:                                                                                                  \n",
            "INFO:absl: concatenate (Concatenate)   (None, 4)                    0         ['culmen_length_mm[0][0]',    \n",
            "INFO:absl:                                                                     'culmen_depth_mm[0][0]',     \n",
            "INFO:absl:                                                                     'flipper_length_mm[0][0]',   \n",
            "INFO:absl:                                                                     'body_mass_g[0][0]']         \n",
            "INFO:absl:                                                                                                  \n",
            "INFO:absl: dense (Dense)               (None, 8)                    40        ['concatenate[0][0]']         \n",
            "INFO:absl:                                                                                                  \n",
            "INFO:absl: dense_1 (Dense)             (None, 8)                    72        ['dense[0][0]']               \n",
            "INFO:absl:                                                                                                  \n",
            "INFO:absl: dense_2 (Dense)             (None, 3)                    27        ['dense_1[0][0]']             \n",
            "INFO:absl:                                                                                                  \n",
            "INFO:absl:==================================================================================================\n",
            "INFO:absl:Total params: 139 (556.00 Byte)\n",
            "INFO:absl:Trainable params: 139 (556.00 Byte)\n",
            "INFO:absl:Non-trainable params: 0 (0.00 Byte)\n",
            "INFO:absl:__________________________________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100/100 [==============================] - 1s 4ms/step - loss: 0.4907 - sparse_categorical_accuracy: 0.8480 - val_loss: 0.1322 - val_sparse_categorical_accuracy: 0.9400\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:absl:Function `_wrapped_model` contains input name(s) resource with unsupported characters which will be renamed to model_dense_2_biasadd_readvariableop_resource in the SavedModel.\n",
            "INFO:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n",
            "INFO:absl:Writing fingerprint to pipelines/penguin-simple/Trainer/model/5/Format-Serving/fingerprint.pb\n",
            "INFO:absl:Training complete. Model written to pipelines/penguin-simple/Trainer/model/5/Format-Serving. ModelRun written to pipelines/penguin-simple/Trainer/model_run/5\n",
            "INFO:absl:Cleaning up stateless execution info.\n",
            "INFO:absl:Execution 5 succeeded.\n",
            "INFO:absl:Cleaning up stateful execution info.\n",
            "INFO:absl:Deleted stateful_working_dir pipelines/penguin-simple/Trainer/.system/stateful_working_dir/4c2c0883-0380-4e7e-9b19-b641f44ad4a9\n",
            "INFO:absl:Publishing output artifacts defaultdict(<class 'list'>, {'model_run': [Artifact(artifact: uri: \"pipelines/penguin-simple/Trainer/model_run/5\"\n",
            ", artifact_type: name: \"ModelRun\"\n",
            ")], 'model': [Artifact(artifact: uri: \"pipelines/penguin-simple/Trainer/model/5\"\n",
            ", artifact_type: name: \"Model\"\n",
            "base_type: MODEL\n",
            ")]}) for execution 5\n",
            "INFO:absl:MetadataStore with DB connection initialized\n",
            "INFO:absl:Component Trainer is finished.\n",
            "INFO:absl:Component Pusher is running.\n",
            "INFO:absl:Running launcher for node_info {\n",
            "  type {\n",
            "    name: \"tfx.components.pusher.component.Pusher\"\n",
            "    base_type: DEPLOY\n",
            "  }\n",
            "  id: \"Pusher\"\n",
            "}\n",
            "contexts {\n",
            "  contexts {\n",
            "    type {\n",
            "      name: \"pipeline\"\n",
            "    }\n",
            "    name {\n",
            "      field_value {\n",
            "        string_value: \"penguin-simple\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  contexts {\n",
            "    type {\n",
            "      name: \"pipeline_run\"\n",
            "    }\n",
            "    name {\n",
            "      field_value {\n",
            "        string_value: \"2024-08-22T06:20:44.717237\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  contexts {\n",
            "    type {\n",
            "      name: \"node\"\n",
            "    }\n",
            "    name {\n",
            "      field_value {\n",
            "        string_value: \"penguin-simple.Pusher\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "inputs {\n",
            "  inputs {\n",
            "    key: \"model\"\n",
            "    value {\n",
            "      channels {\n",
            "        producer_node_query {\n",
            "          id: \"Trainer\"\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"pipeline\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"penguin-simple\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"pipeline_run\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"2024-08-22T06:20:44.717237\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"node\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"penguin-simple.Trainer\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        artifact_query {\n",
            "          type {\n",
            "            name: \"Model\"\n",
            "            base_type: MODEL\n",
            "          }\n",
            "        }\n",
            "        output_key: \"model\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "outputs {\n",
            "  outputs {\n",
            "    key: \"pushed_model\"\n",
            "    value {\n",
            "      artifact_spec {\n",
            "        type {\n",
            "          name: \"PushedModel\"\n",
            "          base_type: MODEL\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "parameters {\n",
            "  parameters {\n",
            "    key: \"custom_config\"\n",
            "    value {\n",
            "      field_value {\n",
            "        string_value: \"null\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  parameters {\n",
            "    key: \"push_destination\"\n",
            "    value {\n",
            "      field_value {\n",
            "        string_value: \"{\\n  \\\"filesystem\\\": {\\n    \\\"base_directory\\\": \\\"serving_model/penguin-simple\\\"\\n  }\\n}\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "upstream_nodes: \"Trainer\"\n",
            "execution_options {\n",
            "  caching_options {\n",
            "  }\n",
            "}\n",
            "\n",
            "INFO:absl:MetadataStore with DB connection initialized\n",
            "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
            "INFO:absl:[Pusher] Resolved inputs: ({'model': [Artifact(artifact: id: 7\n",
            "type_id: 17\n",
            "uri: \"pipelines/penguin-simple/Trainer/model/5\"\n",
            "custom_properties {\n",
            "  key: \"is_external\"\n",
            "  value {\n",
            "    int_value: 0\n",
            "  }\n",
            "}\n",
            "custom_properties {\n",
            "  key: \"tfx_version\"\n",
            "  value {\n",
            "    string_value: \"1.15.1\"\n",
            "  }\n",
            "}\n",
            "state: LIVE\n",
            "type: \"Model\"\n",
            "create_time_since_epoch: 1724307661150\n",
            "last_update_time_since_epoch: 1724307661150\n",
            ", artifact_type: id: 17\n",
            "name: \"Model\"\n",
            "base_type: MODEL\n",
            ")]},)\n",
            "INFO:absl:MetadataStore with DB connection initialized\n",
            "INFO:absl:Going to run a new execution 6\n",
            "INFO:absl:Going to run a new execution: ExecutionInfo(execution_id=6, input_dict={'model': [Artifact(artifact: id: 7\n",
            "type_id: 17\n",
            "uri: \"pipelines/penguin-simple/Trainer/model/5\"\n",
            "custom_properties {\n",
            "  key: \"is_external\"\n",
            "  value {\n",
            "    int_value: 0\n",
            "  }\n",
            "}\n",
            "custom_properties {\n",
            "  key: \"tfx_version\"\n",
            "  value {\n",
            "    string_value: \"1.15.1\"\n",
            "  }\n",
            "}\n",
            "state: LIVE\n",
            "type: \"Model\"\n",
            "create_time_since_epoch: 1724307661150\n",
            "last_update_time_since_epoch: 1724307661150\n",
            ", artifact_type: id: 17\n",
            "name: \"Model\"\n",
            "base_type: MODEL\n",
            ")]}, output_dict=defaultdict(<class 'list'>, {'pushed_model': [Artifact(artifact: uri: \"pipelines/penguin-simple/Pusher/pushed_model/6\"\n",
            ", artifact_type: name: \"PushedModel\"\n",
            "base_type: MODEL\n",
            ")]}), exec_properties={'push_destination': '{\\n  \"filesystem\": {\\n    \"base_directory\": \"serving_model/penguin-simple\"\\n  }\\n}', 'custom_config': 'null'}, execution_output_uri='pipelines/penguin-simple/Pusher/.system/executor_execution/6/executor_output.pb', stateful_working_dir='pipelines/penguin-simple/Pusher/.system/stateful_working_dir/fbd5ee47-1684-4337-b1e8-ecfacddf48d7', tmp_dir='pipelines/penguin-simple/Pusher/.system/executor_execution/6/.temp/', pipeline_node=node_info {\n",
            "  type {\n",
            "    name: \"tfx.components.pusher.component.Pusher\"\n",
            "    base_type: DEPLOY\n",
            "  }\n",
            "  id: \"Pusher\"\n",
            "}\n",
            "contexts {\n",
            "  contexts {\n",
            "    type {\n",
            "      name: \"pipeline\"\n",
            "    }\n",
            "    name {\n",
            "      field_value {\n",
            "        string_value: \"penguin-simple\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  contexts {\n",
            "    type {\n",
            "      name: \"pipeline_run\"\n",
            "    }\n",
            "    name {\n",
            "      field_value {\n",
            "        string_value: \"2024-08-22T06:20:44.717237\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  contexts {\n",
            "    type {\n",
            "      name: \"node\"\n",
            "    }\n",
            "    name {\n",
            "      field_value {\n",
            "        string_value: \"penguin-simple.Pusher\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "inputs {\n",
            "  inputs {\n",
            "    key: \"model\"\n",
            "    value {\n",
            "      channels {\n",
            "        producer_node_query {\n",
            "          id: \"Trainer\"\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"pipeline\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"penguin-simple\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"pipeline_run\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"2024-08-22T06:20:44.717237\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        context_queries {\n",
            "          type {\n",
            "            name: \"node\"\n",
            "          }\n",
            "          name {\n",
            "            field_value {\n",
            "              string_value: \"penguin-simple.Trainer\"\n",
            "            }\n",
            "          }\n",
            "        }\n",
            "        artifact_query {\n",
            "          type {\n",
            "            name: \"Model\"\n",
            "            base_type: MODEL\n",
            "          }\n",
            "        }\n",
            "        output_key: \"model\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "outputs {\n",
            "  outputs {\n",
            "    key: \"pushed_model\"\n",
            "    value {\n",
            "      artifact_spec {\n",
            "        type {\n",
            "          name: \"PushedModel\"\n",
            "          base_type: MODEL\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "parameters {\n",
            "  parameters {\n",
            "    key: \"custom_config\"\n",
            "    value {\n",
            "      field_value {\n",
            "        string_value: \"null\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "  parameters {\n",
            "    key: \"push_destination\"\n",
            "    value {\n",
            "      field_value {\n",
            "        string_value: \"{\\n  \\\"filesystem\\\": {\\n    \\\"base_directory\\\": \\\"serving_model/penguin-simple\\\"\\n  }\\n}\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "upstream_nodes: \"Trainer\"\n",
            "execution_options {\n",
            "  caching_options {\n",
            "  }\n",
            "}\n",
            ", pipeline_info=id: \"penguin-simple\"\n",
            ", pipeline_run_id='2024-08-22T06:20:44.717237', top_level_pipeline_run_id=None, frontend_url=None)\n",
            "WARNING:absl:Pusher is going to push the model without validation. Consider using Evaluator or InfraValidator in your pipeline.\n",
            "INFO:absl:Model version: 1724307661\n",
            "INFO:absl:Model written to serving path serving_model/penguin-simple/1724307661.\n",
            "INFO:absl:Model pushed to pipelines/penguin-simple/Pusher/pushed_model/6.\n",
            "INFO:absl:Cleaning up stateless execution info.\n",
            "INFO:absl:Execution 6 succeeded.\n",
            "INFO:absl:Cleaning up stateful execution info.\n",
            "INFO:absl:Deleted stateful_working_dir pipelines/penguin-simple/Pusher/.system/stateful_working_dir/fbd5ee47-1684-4337-b1e8-ecfacddf48d7\n",
            "INFO:absl:Publishing output artifacts defaultdict(<class 'list'>, {'pushed_model': [Artifact(artifact: uri: \"pipelines/penguin-simple/Pusher/pushed_model/6\"\n",
            ", artifact_type: name: \"PushedModel\"\n",
            "base_type: MODEL\n",
            ")]}) for execution 6\n",
            "INFO:absl:MetadataStore with DB connection initialized\n",
            "INFO:absl:Component Pusher is finished.\n"
          ]
        }
      ],
      "source": [
        "tfx.orchestration.LocalDagRunner().run(\n",
        "  _create_pipeline(\n",
        "      pipeline_name=PIPELINE_NAME,\n",
        "      pipeline_root=PIPELINE_ROOT,\n",
        "      data_root=DATA_ROOT,\n",
        "      module_file=_trainer_module_file,\n",
        "      serving_model_dir=SERVING_MODEL_DIR,\n",
        "      metadata_path=METADATA_PATH))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-05-08T09:40:35.672842Z",
          "iopub.status.busy": "2024-05-08T09:40:35.672553Z",
          "iopub.status.idle": "2024-05-08T09:40:35.843660Z",
          "shell.execute_reply": "2024-05-08T09:40:35.842561Z"
        },
        "id": "NTHROkqX6yHx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c300dd5a-76fe-4c27-c88c-7dcec25d94ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "serving_model/penguin-simple\n",
            "serving_model/penguin-simple/1724305727\n",
            "serving_model/penguin-simple/1724305727/variables\n",
            "serving_model/penguin-simple/1724305727/variables/variables.index\n",
            "serving_model/penguin-simple/1724305727/variables/variables.data-00000-of-00001\n",
            "serving_model/penguin-simple/1724305727/keras_metadata.pb\n",
            "serving_model/penguin-simple/1724305727/fingerprint.pb\n",
            "serving_model/penguin-simple/1724305727/saved_model.pb\n",
            "serving_model/penguin-simple/1724305727/assets\n",
            "serving_model/penguin-simple/1724307661\n",
            "serving_model/penguin-simple/1724307661/variables\n",
            "serving_model/penguin-simple/1724307661/variables/variables.index\n",
            "serving_model/penguin-simple/1724307661/variables/variables.data-00000-of-00001\n",
            "serving_model/penguin-simple/1724307661/keras_metadata.pb\n",
            "serving_model/penguin-simple/1724307661/fingerprint.pb\n",
            "serving_model/penguin-simple/1724307661/saved_model.pb\n",
            "serving_model/penguin-simple/1724307661/assets\n"
          ]
        }
      ],
      "source": [
        "# List files in created model directory.\n",
        "!find {SERVING_MODEL_DIR}"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "DjUA6S30k52h"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}